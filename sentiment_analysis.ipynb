{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import time\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review Sentiments: [('Decent Chinese Food.', 'POSITIVE', 0.9998672008514404), ('The Hunan items are my favorite.', 'POSITIVE', 0.9989293217658997)]\n",
      "\n",
      "Review Sentiments: [(\"We've been coming here for well over 15 years.\", 'POSITIVE', 0.8908674120903015), ('The management has changed once or twice, but it remains our favorite Chinese food spot in Folsom.', 'POSITIVE', 0.9993718266487122), ('My favorites are the wonton soup and General chicken.', 'POSITIVE', 0.9910495281219482), ('A+ customer service and great food at great prices.', 'POSITIVE', 0.9997970461845398)]\n",
      "\n",
      "Review Sentiments: [('Great food, decent prices.', 'POSITIVE', 0.9998748302459717), ('You get A lot of food w/e you order.', 'POSITIVE', 0.997488260269165), ('The dinner or lunch combos are a good choice.', 'POSITIVE', 0.999488115310669)]\n",
      "\n",
      "Review Sentiments: [('Another great meal.', 'POSITIVE', 0.9998217225074768), ('Great service dine in or delivery.', 'POSITIVE', 0.9840313196182251), ('China House is our go to.', 'POSITIVE', 0.9993076324462891)]\n",
      "\n",
      "Review Sentiments: [('Great authentic sezchuan style food.', 'POSITIVE', 0.9998354911804199), ('Family style or a la carte.', 'POSITIVE', 0.9865041971206665), ('Literally my favorite restaurant in Penn Valley!', 'POSITIVE', 0.9992226362228394)]\n",
      "\n",
      "Review Sentiments: [('Thanked me for coming in, very friendly, ready with the food in 15 min. for takeout and it was delicious!', 'POSITIVE', 0.9998483657836914), (\"Great food and friendly service, I'd recommend to anybody.\", 'POSITIVE', 0.9998757839202881)]\n",
      "\n",
      "Review Sentiments: [('Great food and service at a reasonable price', 'POSITIVE', 0.9998353719711304)]\n",
      "\n",
      "Review Sentiments: [('Friendly service and tasty, inexpensive food.', 'POSITIVE', 0.9998658895492554)]\n",
      "\n",
      "Review Sentiments: [('Great prices for quality food.', 'POSITIVE', 0.999580442905426), ('Service is A+ too.', 'POSITIVE', 0.996791422367096)]\n",
      "\n",
      "Review Sentiments: [('Great customer service!', 'POSITIVE', 0.9998698234558105), ('Food is not that great.', 'NEGATIVE', 0.9996688365936279), ('25$ for three items and it taste tasteless.', 'NEGATIVE', 0.9995088577270508), ('And found a hair in soup.', 'NEGATIVE', 0.9793070554733276)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Spacy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load sentiment analysis pipeline from transformers\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def process_review(review):\n",
    "    # Tokenize the review into sentences\n",
    "    doc = nlp(review)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "    # Perform sentiment analysis on each sentence\n",
    "    sentiments = []\n",
    "    for sentence in sentences:\n",
    "        result = sentiment_pipeline(sentence)[0]\n",
    "        sentiments.append((sentence, result['label'], result['score']))\n",
    "\n",
    "    return sentiments\n",
    "\n",
    "# Replace 'your_file.json' with the path to your JSON file\n",
    "with open('data/ys-reviews-restaurants.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    data = data[0:10]\n",
    "# Process each review\n",
    "for item in data:\n",
    "    review = item['text']\n",
    "    sentiments = process_review(review)\n",
    "    print(f\"Review Sentiments: {sentiments}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews to process: 678759\n",
      "Processed 0 reviews.\n",
      "Processed 10000 reviews.\n",
      "Processed 20000 reviews.\n",
      "Processed 30000 reviews.\n",
      "Processed 40000 reviews.\n",
      "Processed 50000 reviews.\n",
      "Processed 60000 reviews.\n",
      "Processed 70000 reviews.\n",
      "Processed 80000 reviews.\n",
      "Processed 90000 reviews.\n",
      "Processed 100000 reviews.\n",
      "Processed 110000 reviews.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (880 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence too long in review 119007: (Original)\n",
      "อาหารอร่อยบริการดีพนักงานยิ้มแย้มแจ่มใส แต่กว่าจะได้เสิร์ฟได้กินนานมากครับ แล้วนี่เป็นวันพุธไม่ใช่ศุกร์เสาร์อาทิตย์เหมือนกับพนักงานไม่พอครับ หรือไม่มีมือครัวเพราะว่ากว่าจะได้อาหารเหมือนกับไปหั่นผักหั่นเนื้ออยู่ๆธรรมดาร้านอาหารต้องคั่วไปเลยมือครัวเขาจะตัดไว้ให้แล้ว เดี๋ยวไปฝากลูกหลานเป็นมือครัวไหมครับ คิดว่าอาหารอร่อยจนขายดีครับแขกมาเต็มเลยแต่เขาก็เซ็งรอกันนั่นแหละ ผมอ่านรีวิวในบอร์ดนี้เขาก็พูดประมาณเหมือนกันนั่นแหละอาหารอร่อยแต่ว่ารอนาน\n",
      "Processed 120000 reviews.\n",
      "Processed 130000 reviews.\n",
      "Processed 140000 reviews.\n",
      "Processed 150000 reviews.\n",
      "Processed 160000 reviews.\n",
      "Processed 170000 reviews.\n",
      "Processed 180000 reviews.\n",
      "Processed 190000 reviews.\n",
      "Processed 200000 reviews.\n",
      "Processed 210000 reviews.\n",
      "Processed 220000 reviews.\n",
      "Processed 230000 reviews.\n",
      "Processed 240000 reviews.\n",
      "Processed 250000 reviews.\n",
      "Processed 260000 reviews.\n",
      "Processed 270000 reviews.\n",
      "Processed 280000 reviews.\n",
      "Processed 290000 reviews.\n",
      "Processed 300000 reviews.\n",
      "Processed 310000 reviews.\n",
      "Processed 320000 reviews.\n",
      "Processed 330000 reviews.\n",
      "Processed 340000 reviews.\n",
      "Processed 350000 reviews.\n",
      "Processed 360000 reviews.\n",
      "Processed 370000 reviews.\n",
      "Processed 380000 reviews.\n",
      "Processed 390000 reviews.\n",
      "Processed 400000 reviews.\n",
      "Sentence too long in review 409778: (Translated by Google) Liiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiittttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt.\n",
      "\n",
      "(Original)\n",
      "Liiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiittttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt.\n",
      "Processed 410000 reviews.\n",
      "Processed 420000 reviews.\n",
      "Processed 430000 reviews.\n",
      "Processed 440000 reviews.\n",
      "Processed 450000 reviews.\n",
      "Processed 460000 reviews.\n",
      "Processed 470000 reviews.\n",
      "Processed 480000 reviews.\n",
      "Processed 490000 reviews.\n",
      "Processed 500000 reviews.\n",
      "Processed 510000 reviews.\n",
      "Processed 520000 reviews.\n",
      "Processed 530000 reviews.\n",
      "Processed 540000 reviews.\n",
      "Processed 550000 reviews.\n",
      "Processed 560000 reviews.\n",
      "Processed 570000 reviews.\n",
      "Processed 580000 reviews.\n",
      "Processed 590000 reviews.\n",
      "Processed 600000 reviews.\n",
      "Processed 610000 reviews.\n",
      "Processed 620000 reviews.\n",
      "Processed 630000 reviews.\n",
      "Processed 640000 reviews.\n",
      "Processed 650000 reviews.\n",
      "Processed 660000 reviews.\n",
      "Processed 670000 reviews.\n",
      "Processing complete. Data saved in 'processed_restaurant_reviews.json'\n"
     ]
    }
   ],
   "source": [
    "# Load Spacy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "model_name = \"siebert/sentiment-roberta-large-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Check if CUDA (GPU support) is available and use it\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "def process_review(review_text, review_id):\n",
    "    doc = nlp(review_text)\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        # Check if the sentence is too long\n",
    "        tokens = tokenizer.encode(sent.text, add_special_tokens=True)\n",
    "        if len(tokens) > 512:\n",
    "            print(f\"Sentence too long in review {review_id}: {sent.text}\")\n",
    "            continue  # Skip this sentence\n",
    "\n",
    "        sentiment_result = sentiment_pipeline(sent.text)[0]\n",
    "        sentences.append({\n",
    "            \"text\": sent.text.strip(),\n",
    "            \"topics\": [],  # Placeholder for topics, to be filled later\n",
    "            \"sentiment\": sentiment_result['label']\n",
    "        })\n",
    "\n",
    "    if review_id % 10000 == 0:\n",
    "        print(f\"Processed {review_id} reviews.\")\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Function to load a hits file and return a structured dictionary\n",
    "def load_hits_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        hits_data = json.load(file)\n",
    "    hits_dict = {}\n",
    "    for hit in hits_data:\n",
    "        doc_index = hit['doc_index']\n",
    "        sentence_index = hit['sentence_index']\n",
    "        if doc_index not in hits_dict:\n",
    "            hits_dict[doc_index] = {}\n",
    "        hits_dict[doc_index][sentence_index] = hit['lemma']\n",
    "    return hits_dict\n",
    "\n",
    "# Load main reviews file\n",
    "with open('data/ys-reviews-restaurants.json', 'r') as file:\n",
    "    reviews_data = json.load(file)\n",
    "\n",
    "# Display total number of reviews\n",
    "print(f\"Total reviews to process: {len(reviews_data)}\")\n",
    "\n",
    "# Load topic hits files\n",
    "topics = [\"clean\", \"food\", \"location\", \"price\", \"service\"]\n",
    "hits_files = [f\"data/topics/{topic}-hits-restaurant-reviews.json\" for topic in topics]\n",
    "hits_dicts = {topic: load_hits_file(file) for topic, file in zip(topics, hits_files)}\n",
    "\n",
    "# Process and structure the reviews data\n",
    "restaurants = {}\n",
    "for i, review in enumerate(reviews_data):\n",
    "    gmap_id = review[\"gmap_id\"]\n",
    "    if gmap_id not in restaurants:\n",
    "        restaurants[gmap_id] = {\"reviews\": []}\n",
    "    processed_sentences = process_review(review[\"text\"], i)\n",
    "    for j, sentence in enumerate(processed_sentences):\n",
    "        for topic, hits_dict in hits_dicts.items():\n",
    "            if i in hits_dict and j in hits_dict[i]:\n",
    "                sentence[\"topics\"].append(topic)\n",
    "    restaurants[gmap_id][\"reviews\"].append({\"sentences\": processed_sentences})\n",
    "\n",
    "# Convert the structured data to the desired format\n",
    "final_structure = {\"restaurants\": [{\"gmap_id\": gmap_id, \"reviews\": data[\"reviews\"]} for gmap_id, data in restaurants.items()]}\n",
    "\n",
    "# Write to a new JSON file\n",
    "with open('processed_restaurant_reviews.json', 'w') as outfile:\n",
    "    json.dump(final_structure, outfile, indent=4)\n",
    "\n",
    "print(\"Processing complete. Data saved in 'processed_restaurant_reviews.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing different model and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews in the file: 678759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3057 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected sentences for 1000000 reviews.\n",
      "Processed sentiment analysis for 1629759 sentences in 10797.93 seconds.\n",
      "Processing complete. Data saved in 'processed_restaurant_reviews_2.json'\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model from Hugging Face\n",
    "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Check if CUDA (GPU support) is available and use it\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "def process_reviews(reviews_data, max_reviews=1000000):\n",
    "    all_sentences = []\n",
    "    sentence_mappings = []\n",
    "    review_structures = []\n",
    "    batch_size = 10000  # Adjust based on your GPU memory usage\n",
    "\n",
    "    # Collect sentences from the first 'max_reviews' reviews\n",
    "    for review_id, review in enumerate(reviews_data[:max_reviews]):\n",
    "        doc = nlp(review[\"text\"])\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            tokens = tokenizer.encode(sent.text, add_special_tokens=True)\n",
    "            if len(tokens) <= 512:\n",
    "                all_sentences.append(sent.text.strip())\n",
    "                sentence_mappings.append((review_id, len(sentences)))\n",
    "                sentences.append({\n",
    "                    \"text\": sent.text.strip(),\n",
    "                    \"topics\": [],\n",
    "                    \"sentiment\": \"\"\n",
    "                })\n",
    "        review_structures.append(sentences)\n",
    "\n",
    "    print(f\"Collected sentences for {max_reviews} reviews.\")\n",
    "\n",
    "    # Perform sentiment analysis on the collected sentences\n",
    "    start_time = time.time()\n",
    "    for i in range(0, len(all_sentences), batch_size):\n",
    "        batch = all_sentences[i:i + batch_size]\n",
    "        batch_sentiments = sentiment_pipeline(batch)\n",
    "        for j, sentiment_result in enumerate(batch_sentiments):\n",
    "            review_id, sentence_id = sentence_mappings[i + j]\n",
    "            if review_id < len(review_structures) and sentence_id < len(review_structures[review_id]):\n",
    "                review_structures[review_id][sentence_id][\"sentiment\"] = sentiment_result['label']\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed sentiment analysis for {len(all_sentences)} sentences in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    return review_structures\n",
    "\n",
    "def load_hits_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        hits_data = json.load(file)\n",
    "    hits_dict = {}\n",
    "    for hit in hits_data:\n",
    "        doc_index = hit['doc_index']\n",
    "        sentence_index = hit['sentence_index']\n",
    "        if doc_index not in hits_dict:\n",
    "            hits_dict[doc_index] = {}\n",
    "        hits_dict[doc_index][sentence_index] = hit['lemma']\n",
    "    return hits_dict\n",
    "\n",
    "# Load main reviews file\n",
    "with open('data/ys-reviews-restaurants.json', 'r') as file:\n",
    "    reviews_data = json.load(file)\n",
    "\n",
    "# Display total number of reviews\n",
    "print(f\"Total reviews in the file: {len(reviews_data)}\")\n",
    "\n",
    "# Load topic hits files\n",
    "topics = [\"clean\", \"food\", \"location\", \"price\", \"service\"]\n",
    "hits_files = [f\"data/topics/{topic}-hits-restaurant-reviews.json\" for topic in topics]\n",
    "hits_dicts = {topic: load_hits_file(file) for topic, file in zip(topics, hits_files)}\n",
    "\n",
    "# Process reviews and obtain structured data\n",
    "review_structures = process_reviews(reviews_data)\n",
    "\n",
    "# Assign topics to sentences in review structures\n",
    "for topic, hits_dict in hits_dicts.items():\n",
    "    for doc_index, sentence_indices in hits_dict.items():  # Change here\n",
    "        for sentence_index in sentence_indices:  # Iterate over sentence indices\n",
    "            if doc_index < len(review_structures) and sentence_index < len(review_structures[doc_index]):\n",
    "                review_structures[doc_index][sentence_index][\"topics\"].append(topic)\n",
    "\n",
    "# Build the final data structure\n",
    "restaurants = {}\n",
    "for i, review in enumerate(review_structures):\n",
    "    gmap_id = reviews_data[i][\"gmap_id\"]\n",
    "    if gmap_id not in restaurants:\n",
    "        restaurants[gmap_id] = {\"reviews\": []}\n",
    "    restaurants[gmap_id][\"reviews\"].append({\"sentences\": review})\n",
    "\n",
    "final_structure = {\"restaurants\": [{\"gmap_id\": gmap_id, \"reviews\": data[\"reviews\"]} for gmap_id, data in restaurants.items()]}\n",
    "\n",
    "# Write to a new JSON file\n",
    "with open('processed_restaurant_reviews_2.json', 'w') as outfile:\n",
    "    json.dump(final_structure, outfile, indent=4)\n",
    "\n",
    "print(\"Processing complete. Data saved in 'processed_restaurant_reviews_2.json'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
